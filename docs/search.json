[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Our Course Project",
    "section": "",
    "text": "I’m honored to be a member of the  RTISTLER  project team.\nBelow, you’ll find a brief summary of our project. To access a detailed project description, please go to https://emu-hacettepe-analytics.github.io/emu430-fall2024-team-rtistler/\nSummary\n We selected the data set on education levels by province from TÜİK (Turkish Statistical Institute). Our reasons for choosing this data are that it provides access to a large amount of information, making it easier for us to visualize and interpret the data. On the other hand, education is crucial for an individual’s development and is essential for both society and the surrounding environment. As university students, we want to observe the state of education in Turkey through concrete data and draw some conclusions. Our objectives are to analyze the changes in Turkey’s education situation over the years and across provinces, in order to: 1-Identify regions where educational opportunities are lacking. 2-Evaluate whether past policies have been e ective. 3-Develop smart policies to ensure equal access to education. 4-Draw conclusions on how education has impacted our society.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello! My name is Emir Madan.\nThis is my personal webpage.\nGet ready, I can share my works on data analytics, blog posts, and more.\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/assignment-2.html",
    "href": "assignments/assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Assignment 2\n#Code Chunk oluşturmaya çalışınca hata veriyor ve renderlamıyor o yüzden malesef burada kullanamadım. #2 #Let’s load libraries\nlibrary(tidyverse) library(rvest) library(stringr)\n\n\nURL’s\nurl1 &lt;- “https://www.imdb.com/search/title/?title_type=feature&release_date=,2009-12-31&num_votes=2500,&country_of_origin=TR&count=250” url2 &lt;- “https://www.imdb.com/search/title/?title_type=feature&release_date=2010-01-01,2023-12-31&num_votes=2500,&country_of_origin=TR&count=250”\n#The function the get the datas from URL’S\n#Function to get the datas from URL’S get_movies &lt;- function(url) { data_html &lt;- read_html(url) Extract the datas with their CSS Selector. I found them with Inspect selection.(I tried SelectorGadget but it didn’t work on IMDB site. I don’t know the reason.) #Extract the titles title_names &lt;- data_html |&gt; html_nodes(’.ipc-title__text’) |&gt; html_text() title_names &lt;- tail(head(title_names,-1),-1) title_names &lt;- str_split(title_names, ” “, n=2) title_names &lt;- unlist(lapply(title_names, function(x) {x[2]})) #Extract the year (same CSS Selector but extract the datas with four digits) year &lt;- data_html |&gt; html_nodes(‘.sc-300a8231-7.eaXxft.dli-title-metadata-item’) |&gt; html_text() year &lt;- str_extract(year,”\\d{4}“) year &lt;- year[!is.na(year)] year &lt;- as.numeric(year)\n\n\nExtract the data with xh ym and xh. Also If there is no ym make the na value 0.\nduration &lt;- data_html |&gt; html_nodes(‘.sc-300a8231-7.eaXxft.dli-title-metadata-item’) |&gt; html_text() duration &lt;- str_extract(duration, “\\d+h \\d+m|\\d+h”) duration &lt;- duration[!is.na(duration)] hours &lt;- as.numeric(str_extract(duration, “\\d+(?=h)”)) minutes &lt;- as.numeric(str_extract(duration, “\\d+(?=m)”)) minutes[is.na(minutes)] &lt;- 0 duration &lt;- hours * 60 + minutes #Extract ratings and make them numeric. rating &lt;- data_html |&gt; html_nodes(‘.ipc-rating-star–rating’) |&gt; html_text() rating &lt;- str_extract(rating, “\\d+(\\.\\d+)?”) rating &lt;- as.numeric(rating) #Extract votes. Remove brackets, K letter and .(dot). After that make them numeric and multiple with 1000 vote &lt;- data_html |&gt; html_nodes(‘.ipc-rating-star–voteCount’) |&gt; html_text() vote &lt;- str_squish(vote) vote &lt;- str_remove_all(vote, “\\.”) vote &lt;- str_remove(vote, “^\\(”) vote &lt;- str_remove(vote, “\\)\\(\")\n  vote &lt;- str_replace(vote, \"K\\)”, ““) vote &lt;- as.numeric(vote) vote &lt;- vote * 1000\n#Create data table with the vectors. movies &lt;- tibble( Title = title_names, Year = year, Duration = duration, Rating = rating, Votes = vote ) return(movies) }\n#We are using map_df to take the both URL’S and combine them movies_all &lt;- map_df(c(url1, url2), get_movies)\n#Check first rows of data head(movies_all)\n3 a # Sort data frame descending by Rating movies_sorted &lt;- movies_all |&gt; arrange(desc(Rating))\n\n\nShow first and last 5 movies\ntop_5 &lt;- movies_sorted |&gt; head(5) bottom_5 &lt;- movies_sorted |&gt; tail(5)\nprint(“Top 5 Movies:”) print(top_5)\nprint(“Bottom 5 Movies:”) print(bottom_5) #I watched Recep İvedik, Babam ve Oğlum and Hababam Sınıfı. I think Recep İvedik should have higher rating level instead of 4.9.\nb My favorites are: Babam ve Oğlum(23), A.R.O.G (120) and Her Şey Çok Güzel Olacak (30)\nc\n#Assuming movies_all contains your dataset, with columns Title, Year, Rating\n\n\nCalculate the yearly rating averages\nmovies_all &lt;- movies_all %&gt;% group_by(Year) %&gt;% summarise(average_rating = mean(Rating, na.rm = TRUE))\n\n\nPlot the average rating over the years using ggplot2\nggplot(movies_all, aes(x = Year, y = average_rating)) + geom_point() + labs(title = “Average Movie Ratings Over the Years”, x = “Year”, y = “Average Rating”) + theme_minimal()\n\n\nCalculate the number of movies per year\nmovies_per_year &lt;- movies_all %&gt;% group_by(Year) %&gt;% summarise(number_of_movies = n())\n\n\nPlot the number of movies over the years\nggplot(movies_per_year, aes(x = Year, y = number_of_movies)) + geom_point(color = “blue”) + labs(title = “Number of Movies Released Over the Years”, x = “Year”, y = “Number of Movies”) + theme_minimal()\n\n\nBox plot of ratings over the years\nggplot(movies_all, aes(x = factor(Year), y = Rating)) + geom_boxplot() + labs(title = “Box Plot of Movie Ratings Over the Years”, x = “Year”, y = “Rating”) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) head(Rating)\nd\n\n\nRemove rows where Votes or Rating are NA\nmovies_clean &lt;- movies_all %&gt;% filter(!is.na(Votes) & !is.na(Rating))\n\n\nCalculate the correlation between Votes and Rating\ncorrelation &lt;- cor(movies_clean\\(Votes, movies_clean\\)Rating)\n\n\nPrint the correlation result\nprint(paste(“Pearson Correlation between Votes and Rating:”, round(correlation, 2)))\n\n\nScatter plot to visualize the relationship between Votes and Rating\nggplot(movies_clean, aes(x = Votes, y = Rating)) + geom_point(alpha = 0.5, color = “blue”) + # Add transparency with alpha labs(title = “Scatter Plot of Votes vs. Rating”, x = “Number of Votes”, y = “Rating”) + theme_minimal()\nThe correlation between movie duration and rating is 0.08 which is a low level of correlation. However it can be observed that movies with fewer votes tend to have slightly higher ratings on average.\ne\n\n\nEnsure that Duration is numeric and filter out rows with NA values in Duration or Rating\nmovies_clean_duration &lt;- movies_all %&gt;% filter(!is.na(Duration) & !is.na(Rating))\n\n\nCalculate the correlation between Duration and Rating\ncorrelation_duration &lt;- cor(movies_clean_duration\\(Duration, movies_clean_duration\\)Rating)\n\n\nPrint the correlation result\nprint(paste(“Pearson Correlation between Duration and Rating:”, round(correlation_duration, 2)))\n\n\nScatter plot to visualize the relationship between Duration and Rating\nggplot(movies_clean_duration, aes(x = Duration, y = Rating)) + geom_point(alpha = 0.5, color = “green”) + # Add transparency with alpha labs(title = “Scatter Plot of Duration vs. Rating”, x = “Duration (in minutes)”, y = “Rating”) + theme_minimal()\nAlthough the correlation is not very strong with a correlation of 0.23 we can observe that there is a slight relationship between duration and rating. Specifically, as the movie duration decreases the rating tends to increase although the relationship is not very strong.\n4 Load the libraries again.\nlibrary(rvest) library(dplyr) library(stringr)\n#URL for top 1000 Turkish movies on IMDb url &lt;- “https://www.imdb.com/search/title/?title_type=feature&groups=top_1000&country_of_origin=TR&count=250&sort=user_rating,desc”\n#Function to scrape the top 1000 Turkish movies from IMDb data_html &lt;- read_html(url)\n#Extract titles title_names &lt;- data_html |&gt; html_nodes(’.ipc-title__text’) |&gt; html_text() title_names &lt;- tail(head(title_names,-1),-1) title_names &lt;- str_split(title_names, ” “, n=2) title_names &lt;- unlist(lapply(title_names, function(x) {x[2]})) head(title_names) length(title_names) #Extract years year &lt;- data_html |&gt; html_nodes(‘.sc-300a8231-7.eaXxft.dli-title-metadata-item’) |&gt; html_text() year &lt;- str_extract(year,”\\d{4}“) year &lt;- year[!is.na(year)] year &lt;- as.numeric(year) length(year) #Create a data frame with Title and Year top_1000_turkish_movies &lt;- tibble( Title = title_names, Year = year )\n#Print the first few rows print(head(top_1000_sorted))\n#Make the join operation new_top_1000_turkish_movies &lt;- inner_join(top_1000_turkish_movies, movies_all, by = title_names) #Sort movies by the rating top_1000_sorted &lt;- top_1000_turkish_movies |&gt; arrange(desc(Rating)) print(top_1000_sorted)\nI tried inner_join function for it but the code did not work, but the first 11 Turkish movies in the top 1000 list are not the same as the first 11 movies in our previous ranking. This is likely because IMDb uses a special filtering method for its top 1000 list, which means not all Turkish films we saw are included in this list, and their ranking is different from the one in our previous ranking.\nI used ChatGPT and lecture notes for all codes(mostly ChatGPT but I created the general structure and understand it and also I made some corrections)\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Buradan CV’me erişebilirsiniz. CV"
  },
  {
    "objectID": "about.html#employements",
    "href": "about.html#employements",
    "title": "About Me",
    "section": "Employements",
    "text": "Employements\n\nMarrakesh Cafe, Busser, 2018\nBaxter’s Fish N Chips, Kitchen-Busser-Foodrunner-Barback, 2024\nBlack Cat Restaurant, Parking Lot Attendant, 2024"
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About Me",
    "section": "Internships",
    "text": "Internships"
  },
  {
    "objectID": "assignments/assignment-1.html",
    "href": "assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "My first assignment has two parts.\n\n\nI choose the Mustafa Baydoğan’s video.\nSummary: The video generally focuses on data analysis and forecasting, which Mustafa Baydoğan is more interested in. He makes exemplary explanations regarding the predictions and accuracy of the predictions, especially in the fields of timber, electricity and retail. Some of the most important things I remember are that, leaving aside technical things, even if they are areas that you are not interested in, understanding the problem in data analysis is as important as solving it. He explains this and many other important things very well with daily life examples.\nMultiple choice question:\nWhich of the following is not a step in the structured problem solving and data analysis process?\nA)Desicion\nB)Impact\nC)Solution Method\nD)Reaction\nE)Data\nAnswer: D\nOpen-ended question\nDo the predictions given to make a decision have to be absolutely accurate?(Yes or no and please explain it).\nAnswer: No it doesn’t have to be. The more accurate the prediction, the better, but we do not need 100% accuracy to make the right decision. Afterwards, we can make the right decisions with proper optimization.\n\n\n\nFirstly I installed the dslabs.\n\n#install.packages(dslabs)\n\nI opened the dslabs with the library command in my script.\n\nlibrary(dslabs)\n\nSecondly, I use the head(data, first x data set) to show the first 10 row of data set. (I use ChatGPT in this part to how to show first 10 row.)\n\nhead(polls_us_election_2016,10)\n\n        state  startdate    enddate\n1        U.S. 2016-11-03 2016-11-06\n2        U.S. 2016-11-01 2016-11-07\n3        U.S. 2016-11-02 2016-11-06\n4        U.S. 2016-11-04 2016-11-07\n5        U.S. 2016-11-03 2016-11-06\n6        U.S. 2016-11-03 2016-11-06\n7        U.S. 2016-11-02 2016-11-06\n8        U.S. 2016-11-03 2016-11-05\n9  New Mexico 2016-11-06 2016-11-06\n10       U.S. 2016-11-04 2016-11-07\n                                                     pollster grade samplesize\n1                                    ABC News/Washington Post    A+       2220\n2                                     Google Consumer Surveys     B      26574\n3                                                       Ipsos    A-       2195\n4                                                      YouGov     B       3677\n5                                            Gravis Marketing    B-      16639\n6  Fox News/Anderson Robbins Research/Shaw & Company Research     A       1295\n7                                     CBS News/New York Times    A-       1426\n8                                NBC News/Wall Street Journal    A-       1282\n9                                                    Zia Poll  &lt;NA&gt;       8439\n10                                                   IBD/TIPP    A-       1107\n   population rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin\n1          lv           47.00         43.00            4.00               NA\n2          lv           38.03         35.69            5.46               NA\n3          lv           42.00         39.00            6.00               NA\n4          lv           45.00         41.00            5.00               NA\n5          rv           47.00         43.00            3.00               NA\n6          lv           48.00         44.00            3.00               NA\n7          lv           45.00         41.00            5.00               NA\n8          lv           44.00         40.00            6.00               NA\n9          lv           46.00         44.00            6.00               NA\n10         lv           41.20         42.70            7.10               NA\n   adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin\n1         45.20163      41.72430        4.626221               NA\n2         43.34557      41.21439        5.175792               NA\n3         42.02638      38.81620        6.844734               NA\n4         45.65676      40.92004        6.069454               NA\n5         46.84089      42.33184        3.726098               NA\n6         49.02208      43.95631        3.057876               NA\n7         45.11649      40.92722        4.341786               NA\n8         43.58576      40.77325        5.365788               NA\n9         44.82594      41.59978        7.870127               NA\n10        42.92745      42.23545        6.316175               NA\n\n\nThirdly, I use cat function to print the total NA values in the entire data set. Normally I was using print but I was able to remove [1] with the cat function to make it look neater.(I use ChatGPT in this part to how to remove [1])\n\ncat(\"Total NA values:\", sum(is.na(polls_us_election_2016)))\n\nTotal NA values: 11604\n\n\nFourthly, I use for loop,if statement and replace function to change the entire data set.(I mostly use ChatGPT in this part. At first, I wanted to do it with ifelse, as it was shown in the course slides, but I got an error such as converting characters to numbers in the columns of some data names. Then, although it could be done in other ways, I especially wanted to do it with the for loop, and this way it was more understandable and smooth in my eyes.)\n\n# get the column names in the data set.\ncolumn_names &lt;- names(polls_us_election_2016)\n\n# use for loop to change NA values\nfor (column in column_names) {\n  if (is.numeric(polls_us_election_2016[[column]])) {\n    # change NA values with 2001 for numerical columns\n    polls_us_election_2016[[column]] &lt;- replace(polls_us_election_2016[[column]], is.na(polls_us_election_2016[[column]]), 2001)\n    \n  } else if (is.character(polls_us_election_2016[[column]])) {\n    # change NA values with Emir for character columns\n    polls_us_election_2016[[column]] &lt;- replace(polls_us_election_2016[[column]], is.na(polls_us_election_2016[[column]]), \"Emir\")\n    \n  } else if (is.factor(polls_us_election_2016[[column]])) {\n    # turn the factor columns into character and change NA values with Emir\n    polls_us_election_2016[[column]] &lt;- as.character(polls_us_election_2016[[column]])\n    polls_us_election_2016[[column]] &lt;- replace(polls_us_election_2016[[column]], is.na(polls_us_election_2016[[column]]), \"Emir\")\n    polls_us_election_2016[[column]] &lt;- as.factor(polls_us_election_2016[[column]]) \n  }\n}\n\nFifthly, I head the first 10 row of the data set again.\n\nhead(polls_us_election_2016,10)\n\n        state  startdate    enddate\n1        U.S. 2016-11-03 2016-11-06\n2        U.S. 2016-11-01 2016-11-07\n3        U.S. 2016-11-02 2016-11-06\n4        U.S. 2016-11-04 2016-11-07\n5        U.S. 2016-11-03 2016-11-06\n6        U.S. 2016-11-03 2016-11-06\n7        U.S. 2016-11-02 2016-11-06\n8        U.S. 2016-11-03 2016-11-05\n9  New Mexico 2016-11-06 2016-11-06\n10       U.S. 2016-11-04 2016-11-07\n                                                     pollster grade samplesize\n1                                    ABC News/Washington Post    A+       2220\n2                                     Google Consumer Surveys     B      26574\n3                                                       Ipsos    A-       2195\n4                                                      YouGov     B       3677\n5                                            Gravis Marketing    B-      16639\n6  Fox News/Anderson Robbins Research/Shaw & Company Research     A       1295\n7                                     CBS News/New York Times    A-       1426\n8                                NBC News/Wall Street Journal    A-       1282\n9                                                    Zia Poll  Emir       8439\n10                                                   IBD/TIPP    A-       1107\n   population rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin\n1          lv           47.00         43.00            4.00             2001\n2          lv           38.03         35.69            5.46             2001\n3          lv           42.00         39.00            6.00             2001\n4          lv           45.00         41.00            5.00             2001\n5          rv           47.00         43.00            3.00             2001\n6          lv           48.00         44.00            3.00             2001\n7          lv           45.00         41.00            5.00             2001\n8          lv           44.00         40.00            6.00             2001\n9          lv           46.00         44.00            6.00             2001\n10         lv           41.20         42.70            7.10             2001\n   adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin\n1         45.20163      41.72430        4.626221             2001\n2         43.34557      41.21439        5.175792             2001\n3         42.02638      38.81620        6.844734             2001\n4         45.65676      40.92004        6.069454             2001\n5         46.84089      42.33184        3.726098             2001\n6         49.02208      43.95631        3.057876             2001\n7         45.11649      40.92722        4.341786             2001\n8         43.58576      40.77325        5.365788             2001\n9         44.82594      41.59978        7.870127             2001\n10        42.92745      42.23545        6.316175             2001",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "My Assignments",
    "section": "",
    "text": "On this page, I showcase the assignment I conducted for the [term and year, e.g. Fall 2024] EMU430 Data Analytics course.\nPlease use left menu to navigate through my assignments.\nThe most recent update to this page was made on October 27, 2024\n\n\n\n Back to top",
    "crumbs": [
      "My Assignments"
    ]
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "This page is under construction.\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/assignment-1.html#b",
    "href": "assignments/assignment-1.html#b",
    "title": "Assignment 1",
    "section": "",
    "text": "Firstly I installed the dslabs.\n\n#install.packages(dslabs)\n\nI opened the dslabs with the library command in my script.\n\nlibrary(dslabs)\n\nSecondly, I use the head(data, first x data set) to show the first 10 row of data set. (I use ChatGPT in this part to how to show first 10 row.)\n\nhead(polls_us_election_2016,10)\n\n        state  startdate    enddate\n1        U.S. 2016-11-03 2016-11-06\n2        U.S. 2016-11-01 2016-11-07\n3        U.S. 2016-11-02 2016-11-06\n4        U.S. 2016-11-04 2016-11-07\n5        U.S. 2016-11-03 2016-11-06\n6        U.S. 2016-11-03 2016-11-06\n7        U.S. 2016-11-02 2016-11-06\n8        U.S. 2016-11-03 2016-11-05\n9  New Mexico 2016-11-06 2016-11-06\n10       U.S. 2016-11-04 2016-11-07\n                                                     pollster grade samplesize\n1                                    ABC News/Washington Post    A+       2220\n2                                     Google Consumer Surveys     B      26574\n3                                                       Ipsos    A-       2195\n4                                                      YouGov     B       3677\n5                                            Gravis Marketing    B-      16639\n6  Fox News/Anderson Robbins Research/Shaw & Company Research     A       1295\n7                                     CBS News/New York Times    A-       1426\n8                                NBC News/Wall Street Journal    A-       1282\n9                                                    Zia Poll  &lt;NA&gt;       8439\n10                                                   IBD/TIPP    A-       1107\n   population rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin\n1          lv           47.00         43.00            4.00               NA\n2          lv           38.03         35.69            5.46               NA\n3          lv           42.00         39.00            6.00               NA\n4          lv           45.00         41.00            5.00               NA\n5          rv           47.00         43.00            3.00               NA\n6          lv           48.00         44.00            3.00               NA\n7          lv           45.00         41.00            5.00               NA\n8          lv           44.00         40.00            6.00               NA\n9          lv           46.00         44.00            6.00               NA\n10         lv           41.20         42.70            7.10               NA\n   adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin\n1         45.20163      41.72430        4.626221               NA\n2         43.34557      41.21439        5.175792               NA\n3         42.02638      38.81620        6.844734               NA\n4         45.65676      40.92004        6.069454               NA\n5         46.84089      42.33184        3.726098               NA\n6         49.02208      43.95631        3.057876               NA\n7         45.11649      40.92722        4.341786               NA\n8         43.58576      40.77325        5.365788               NA\n9         44.82594      41.59978        7.870127               NA\n10        42.92745      42.23545        6.316175               NA\n\n\nThirdly, I use cat function to print the total NA values in the entire data set. Normally I was using print but I was able to remove [1] with the cat function to make it look neater.(I use ChatGPT in this part to how to remove [1])\n\ncat(\"Total NA values:\", sum(is.na(polls_us_election_2016)))\n\nTotal NA values: 11604\n\n\nFourthly, I use for loop,if statement and replace function to change the entire data set.(I mostly use ChatGPT in this part. At first, I wanted to do it with ifelse, as it was shown in the course slides, but I got an error such as converting characters to numbers in the columns of some data names. Then, although it could be done in other ways, I especially wanted to do it with the for loop, and this way it was more understandable and smooth in my eyes.)\n\n# get the column names in the data set.\ncolumn_names &lt;- names(polls_us_election_2016)\n\n# use for loop to change NA values\nfor (column in column_names) {\n  if (is.numeric(polls_us_election_2016[[column]])) {\n    # change NA values with 2001 for numerical columns\n    polls_us_election_2016[[column]] &lt;- replace(polls_us_election_2016[[column]], is.na(polls_us_election_2016[[column]]), 2001)\n    \n  } else if (is.character(polls_us_election_2016[[column]])) {\n    # change NA values with Emir for character columns\n    polls_us_election_2016[[column]] &lt;- replace(polls_us_election_2016[[column]], is.na(polls_us_election_2016[[column]]), \"Emir\")\n    \n  } else if (is.factor(polls_us_election_2016[[column]])) {\n    # turn the factor columns into character and change NA values with Emir\n    polls_us_election_2016[[column]] &lt;- as.character(polls_us_election_2016[[column]])\n    polls_us_election_2016[[column]] &lt;- replace(polls_us_election_2016[[column]], is.na(polls_us_election_2016[[column]]), \"Emir\")\n    polls_us_election_2016[[column]] &lt;- as.factor(polls_us_election_2016[[column]]) \n  }\n}\n\nFifthly, I head the first 10 row of the data set again.\n\nhead(polls_us_election_2016,10)\n\n        state  startdate    enddate\n1        U.S. 2016-11-03 2016-11-06\n2        U.S. 2016-11-01 2016-11-07\n3        U.S. 2016-11-02 2016-11-06\n4        U.S. 2016-11-04 2016-11-07\n5        U.S. 2016-11-03 2016-11-06\n6        U.S. 2016-11-03 2016-11-06\n7        U.S. 2016-11-02 2016-11-06\n8        U.S. 2016-11-03 2016-11-05\n9  New Mexico 2016-11-06 2016-11-06\n10       U.S. 2016-11-04 2016-11-07\n                                                     pollster grade samplesize\n1                                    ABC News/Washington Post    A+       2220\n2                                     Google Consumer Surveys     B      26574\n3                                                       Ipsos    A-       2195\n4                                                      YouGov     B       3677\n5                                            Gravis Marketing    B-      16639\n6  Fox News/Anderson Robbins Research/Shaw & Company Research     A       1295\n7                                     CBS News/New York Times    A-       1426\n8                                NBC News/Wall Street Journal    A-       1282\n9                                                    Zia Poll  Emir       8439\n10                                                   IBD/TIPP    A-       1107\n   population rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin\n1          lv           47.00         43.00            4.00             2001\n2          lv           38.03         35.69            5.46             2001\n3          lv           42.00         39.00            6.00             2001\n4          lv           45.00         41.00            5.00             2001\n5          rv           47.00         43.00            3.00             2001\n6          lv           48.00         44.00            3.00             2001\n7          lv           45.00         41.00            5.00             2001\n8          lv           44.00         40.00            6.00             2001\n9          lv           46.00         44.00            6.00             2001\n10         lv           41.20         42.70            7.10             2001\n   adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin\n1         45.20163      41.72430        4.626221             2001\n2         43.34557      41.21439        5.175792             2001\n3         42.02638      38.81620        6.844734             2001\n4         45.65676      40.92004        6.069454             2001\n5         46.84089      42.33184        3.726098             2001\n6         49.02208      43.95631        3.057876             2001\n7         45.11649      40.92722        4.341786             2001\n8         43.58576      40.77325        5.365788             2001\n9         44.82594      41.59978        7.870127             2001\n10        42.92745      42.23545        6.316175             2001",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#a",
    "href": "assignments/assignment-1.html#a",
    "title": "Assignment 1",
    "section": "",
    "text": "I choose the Mustafa Baydoğan’s video.\nSummary: The video generally focuses on data analysis and forecasting, which Mustafa Baydoğan is more interested in. He makes exemplary explanations regarding the predictions and accuracy of the predictions, especially in the fields of timber, electricity and retail. Some of the most important things I remember are that, leaving aside technical things, even if they are areas that you are not interested in, understanding the problem in data analysis is as important as solving it. He explains this and many other important things very well with daily life examples.\nMultiple choice question:\nWhich of the following is not a step in the structured problem solving and data analysis process?\nA)Desicion\nB)Impact\nC)Solution Method\nD)Reaction\nE)Data\nAnswer: D\nOpen-ended question\nDo the predictions given to make a decision have to be absolutely accurate?(Yes or no and please explain it).\nAnswer: No it doesn’t have to be. The more accurate the prediction, the better, but we do not need 100% accuracy to make the right decision. Afterwards, we can make the right decisions with proper optimization.",
    "crumbs": [
      "Assignment 1"
    ]
  }
]